{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyVision Video Stream Processing Demonstration\n",
    "=====\n",
    "\n",
    "* Background Subtraction\n",
    "* Motion Detection\n",
    "* Optical Flow\n",
    "* VideoStreamProcessor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyvision as pv\n",
    "import cv2.cv as cv\n",
    "import os\n",
    "data_dir = os.path.dirname(pv.TAZ_VIDEO)  #in the same directory as the TAZ video there are two others\n",
    "taz = pv.TAZ_VIDEO  #tazmanian devil toy\n",
    "cars = os.path.join(data_dir,\"toy_car.m4v\")  #toy cars on a carpet\n",
    "bug = os.path.join(data_dir,\"BugsSample.m4v\") #a bug in grass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background Subtraction\n",
    "-----\n",
    "\n",
    "PyVision has a few background subtraction methods including:\n",
    "\n",
    "* Median Filter\n",
    "* Approximate Median Filter\n",
    "* N-frame Differencing\n",
    "* Motion Compensated Frame Differencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#image buffers are required for background models\n",
    "imb = pv.ImageBuffer(N=20)\n",
    "imb.fillBuffer( pv.Video(bug) )\n",
    "\n",
    "#get the foreground mask of the \"key frame\" of\n",
    "# the buffer using four methods\n",
    "mf = pv.MedianFilter(imb)\n",
    "amf = pv.ApproximateMedianFilter(imb)\n",
    "nfd = pv.FrameDifferencer(imb)\n",
    "mcfd = pv.MotionCompensatedFrameDifferencer(imb)\n",
    "x1 = mf.getForegroundMask()\n",
    "x2 = amf.getForegroundMask()\n",
    "x3 = nfd.getForegroundMask()\n",
    "x4 = mcfd.getForegroundMask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where's the bug?\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display results in a montage\n",
    "imbug = imb.getLast()\n",
    "imontage = pv.ImageMontage([imbug, x1,x2,x3,x4], layout=(1,5), tile_size=(320,240))\n",
    "imontage.asImage().show( delay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motion Detection\n",
    "----\n",
    "\n",
    "Pyvision also has a high-level motion detection class that works with any of the\n",
    "background subtraction methods. The motion detector not only finds the foreground\n",
    "mask, but also provides a number of annotation options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vid_bug = pv.Video(bug) #a bug in grass\n",
    "imb = pv.ImageBuffer(N=20)\n",
    "imb.fillBuffer(vid_bug) #we have to fill the buffer first...\n",
    "\n",
    "md = pv.MotionDetector(imageBuff=imb, method=pv.BG_SUBTRACT_AMF)\n",
    "for frame in vid_bug:\n",
    "    md.detect(frame)\n",
    "    if md > -1: \n",
    "        img = md.getAnnotatedImage(showRects=True, showContours=True, showConvexHulls=True)\n",
    "        img.show(\"Motion Detection Sample\", delay=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv.DestroyWindow(\"Motion Detection Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the camera exhibits some ego motion, the **motion-compensated frame differencer**\n",
    "may be the way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vid_car = pv.Video(cars)\n",
    "\n",
    "imb1 = pv.ImageBuffer(N=4)  #fast motion, so use smaller buffer\n",
    "imb2 = pv.ImageBuffer(N=4)\n",
    "while not imb1.isFull():\n",
    "    f = vid_car.next()\n",
    "    imb1.add(f)\n",
    "    imb2.add(f)\n",
    "\n",
    "md1 = pv.MotionDetector(imageBuff=imb1, method=pv.BG_SUBTRACT_MCFD)\n",
    "md2 = pv.MotionDetector(imageBuff=imb2, method=pv.BG_SUBTRACT_AMF)\n",
    "for frame in vid_car:\n",
    "    md1.detect(frame)\n",
    "    md2.detect(frame)\n",
    "    if md2 > -1: \n",
    "        img1 = md1.getForegroundPixels()\n",
    "        #md1.annotateFrame(img1, rect_color='red', contour_color=None, flow_color=None)\n",
    "        img1.annotateLabel(pv.Point(5,5),\"Motion Comp. Frame Diff.\", color='white', \n",
    "                           background='black', font=18)\n",
    "        \n",
    "        img2 = md2.getForegroundPixels() \n",
    "        img2.annotateLabel(pv.Point(5,5),\"Approx Median Filter\", color='white',\n",
    "                           background='black', font=18)\n",
    "        \n",
    "        imontage = pv.ImageMontage([img1,img2], layout=(2,1), tile_size=(320,240))\n",
    "        imontage.asImage().show(\"Motion Detection Comparison\", delay=50)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv.DestroyWindow(\"Motion Detection Comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optical Flow\n",
    "----\n",
    "\n",
    "The pv.OpticalFlow class encapsulates LK optical flow computations and is\n",
    "easy to apply to videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vid_car = pv.Video(cars)\n",
    "of = pv.OpticalFlow()\n",
    "for frame in vid_car:\n",
    "    of.update(frame)\n",
    "    of.annotateFrame(frame)\n",
    "    frame.show(window=\"Optical Flow\", delay=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv.DestroyWindow(\"Optical Flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Callback Function Usage\n",
    "----\n",
    "\n",
    "the video.play() method has a parameter where a callback function (or callable object)\n",
    "can be specified. </br> This is a good way to encapsulate various computations you wish to\n",
    "make on a per-frame basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#callback function to apply to each frame of video\n",
    "# in this case, the optical flow object will be passed\n",
    "# using a keyword parameter \"of=\"\n",
    "def processOF(img, fn, of=None, **kwargs):\n",
    "    of.update(img)\n",
    "    of.annotateFrame(img)\n",
    "    img.show(window=\"Optical Flow\", delay=1)\n",
    "\n",
    "vid = pv.Video(cars)\n",
    "flow_obj = pv.OpticalFlow()\n",
    "\n",
    "#Note that this way of running optical flow on a video\n",
    "# gives the user the pause-and-play interface and annotates\n",
    "# the frame number in the upper left (which can be suppressed if desired)\n",
    "vid.play(window=None, delay=50, onNewFrame=processOF, of=flow_obj)\n",
    "\n",
    "\n",
    "cv.DestroyWindow(\"Optical Flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv.DestroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VideoStreamProcessor (VSP)\n",
    "---\n",
    "\n",
    "A VideoStreamProcessor is an object that can be used as video callback function. </br>\n",
    "One use of VSP's is to chain together a set of processes in a modular way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vid = pv.Video(cars)\n",
    "\n",
    "#video writer VSP\n",
    "vwVSP = pv.VideoWriterVSP(\"./demo.avi\",fourCC_str=\"XVID\", fps=15, size=(480,272))\n",
    "\n",
    "#motion detection VSP\n",
    "md = pv.MotionDetector(buffSize=5, method=pv.BG_SUBTRACT_MCFD)\n",
    "mdVSP = pv.MotionDetectionVSP(md, window=\"Motion\", nextModule=vwVSP)\n",
    "\n",
    "#VSP that simply annotates the frame number\n",
    "fnVSP = pv.FrameNumberVSP(window=None, nextModule=mdVSP)\n",
    "\n",
    "# frame number annotation -> motion detection -> video writer\n",
    "vid.play(window=None, delay=33, annotate=None, onNewFrame=fnVSP)\n",
    "\n",
    "cv.DestroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we playback the video we just saved using the video writer VSP\n",
    "vid = pv.Video(\"./demo.avi\")\n",
    "vid.play(window=\"Input\",annotate=None, delay=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People Detection using OpenCV\n",
    "-----\n",
    "\n",
    "This demonstration shows using the HOG Detector in OpenCV to detect people in an image,\n",
    "and how to use the results to annotate the image in pyvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAUSED: Select <a>bort program, <q>uit playback, <c>ontinue playback, or <s>tep to next frame.\n"
     ]
    }
   ],
   "source": [
    "import cv2.cv as cv\n",
    "import pyvision as pv\n",
    "\n",
    "UTI_vid = pv.Video(\"./seq3.avi\") #sample video from UT Interactions data set\n",
    "\n",
    "def detectPeople(img, fn, **kwargs):\n",
    "    if fn < 50: return  #no detections in this video until after frame 50\n",
    "    \n",
    "    cvim = img.asOpenCV()  #convert to OpenCV format before using OpenCV functions\n",
    "    try:\n",
    "        found = list(cv.HOGDetectMultiScale(cvim, cv.CreateMemStorage(0)))\n",
    "        rect_list = [ pv.Rect(x,y,w,h) for ((x,y),(w,h)) in found]  #python list comprehension\n",
    "        for r in rect_list: img.annotateRect(r)\n",
    "        img.show(window=\"People Detection\", delay=1)\n",
    "    except:\n",
    "        #cv.HOGDetectMultiScale can throw exceptions, so catch and try the next frame.\n",
    "        pass\n",
    "    \n",
    "UTI_vid.play(window=None, delay=10, onNewFrame=detectPeople)\n",
    "cv.DestroyWindow(\"People Detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People Detection using PyVision VSPs\n",
    "----\n",
    "\n",
    "We have some VSPs that can do the trick as well. Note the use of **ResizerVSP** to make the window\n",
    "smaller, and thus speed up processing of the people detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyvision as pv\n",
    "import cv2.cv as cv\n",
    "UTI_vid = pv.Video(\"./seq3.avi\") #sample video from UT Interactions data set\n",
    "\n",
    "fnVSP = pv.FrameNumberVSP(window=\"People Detection\")\n",
    "pdVSP = pv.PeopleDetectionVSP(window=None, nextModule=fnVSP)\n",
    "rszVSP = pv.ResizerVSP(new_size=(320,240), window=None, nextModule=pdVSP)\n",
    "\n",
    "#input -> resized -> people detected -> frame number annotated\n",
    "UTI_vid.play(window=None, delay=10, onNewFrame=rszVSP)\n",
    "\n",
    "cv.DestroyWindow(\"People Detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv.DestroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
